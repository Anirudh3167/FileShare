{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76dfcc2386ea4e2a9622cc9e77b6b94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1235bc806d14a9d994fdb94dd497f42",
              "IPY_MODEL_9896b5c1aa48402ca3e5b9caeabe0960",
              "IPY_MODEL_e44c8565cea044e99dd005bc8a7666ca"
            ],
            "layout": "IPY_MODEL_182ac8acc1c84781b7d191d0c6004655"
          }
        },
        "e1235bc806d14a9d994fdb94dd497f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_099ca037e1c7460ab39f40c56ea7ee31",
            "placeholder": "​",
            "style": "IPY_MODEL_1b271ac3514a4a5fb3afebbbeabb8a82",
            "value": "Extracting: 100%"
          }
        },
        "9896b5c1aa48402ca3e5b9caeabe0960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c92d92c2424551bffa3b51796ecde4",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba2eb5e1942e4ca7983a7754bc62e5b4",
            "value": 5000
          }
        },
        "e44c8565cea044e99dd005bc8a7666ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d56c0e6f23d48ec868595e7cccb57e9",
            "placeholder": "​",
            "style": "IPY_MODEL_c7c2f65df7b94a578e077501f532efc3",
            "value": " 5000/5000 [00:19&lt;00:00, 276.82it/s]"
          }
        },
        "182ac8acc1c84781b7d191d0c6004655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "099ca037e1c7460ab39f40c56ea7ee31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b271ac3514a4a5fb3afebbbeabb8a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8c92d92c2424551bffa3b51796ecde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2eb5e1942e4ca7983a7754bc62e5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "2d56c0e6f23d48ec868595e7cccb57e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c2f65df7b94a578e077501f532efc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7063e0500efa45d38f981762aa385cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd9384c4299c40c8920b31de9f942c18",
              "IPY_MODEL_fd85e92c3ee842f18e8f94752d6f7df7",
              "IPY_MODEL_ea462aaac31841a6b0e13677f8fa0edc"
            ],
            "layout": "IPY_MODEL_3a35e4be59804598ab67ab10e5b6c2b7"
          }
        },
        "cd9384c4299c40c8920b31de9f942c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eb3999439bc40f482601e923cea9f6c",
            "placeholder": "​",
            "style": "IPY_MODEL_493e7e24c86c47e881fde5917ba4a5e7",
            "value": "Extracting: 100%"
          }
        },
        "fd85e92c3ee842f18e8f94752d6f7df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_881ff3efd165440baeb1a45ebb25e7a7",
            "max": 549,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca130dd156e34173a05ce06b434a21e7",
            "value": 549
          }
        },
        "ea462aaac31841a6b0e13677f8fa0edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a834551562c41938e0e9f8a6a569383",
            "placeholder": "​",
            "style": "IPY_MODEL_0b14ebd25ce7469aac24db8314283aa3",
            "value": " 549/549 [00:01&lt;00:00, 432.53it/s]"
          }
        },
        "3a35e4be59804598ab67ab10e5b6c2b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb3999439bc40f482601e923cea9f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493e7e24c86c47e881fde5917ba4a5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "881ff3efd165440baeb1a45ebb25e7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca130dd156e34173a05ce06b434a21e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "3a834551562c41938e0e9f8a6a569383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b14ebd25ce7469aac24db8314283aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77948ed1014f4b35ab5eae831d6a76c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_521676a1c4964d9886bb0ed1ab1c2d5c",
              "IPY_MODEL_bb4e2750b1cb49c5b38d42fb2a8bada4",
              "IPY_MODEL_ced8c6c2e12b4dc0ac65d56d3ac970f8"
            ],
            "layout": "IPY_MODEL_fba601e774054044b7ebeecb91def517"
          }
        },
        "521676a1c4964d9886bb0ed1ab1c2d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f99ccc3f273646aa851a6cbc13a28aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_e89068fd94274e2fa031cd31399d7d98",
            "value": "Training:  33%"
          }
        },
        "bb4e2750b1cb49c5b38d42fb2a8bada4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a1015182624b9ca0c62f60dab08b04",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba3ce5ee9e6456d8616b44278adb564",
            "value": 114
          }
        },
        "ced8c6c2e12b4dc0ac65d56d3ac970f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d478d09018be4d77a084eb510651f9a8",
            "placeholder": "​",
            "style": "IPY_MODEL_fd481410ff24402bba261ef0e7cc16a3",
            "value": " 114/349 [16:55&lt;33:07,  8.46s/it, loss=2.01, acc=12.6]"
          }
        },
        "fba601e774054044b7ebeecb91def517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99ccc3f273646aa851a6cbc13a28aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e89068fd94274e2fa031cd31399d7d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a1015182624b9ca0c62f60dab08b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba3ce5ee9e6456d8616b44278adb564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "d478d09018be4d77a084eb510651f9a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd481410ff24402bba261ef0e7cc16a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "793b7aeec0ae48519831d9d17e66ba7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f456707bfad44722becd160370ae6139",
              "IPY_MODEL_bec5ac06dfdd4557a559fd32033567d6",
              "IPY_MODEL_196af022b4e94ad894d194e33d83eb37"
            ],
            "layout": "IPY_MODEL_0b3b1230951f4160a01866047d0c13e9"
          }
        },
        "f456707bfad44722becd160370ae6139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_303e6f5cad92405090de073cd3291100",
            "placeholder": "​",
            "style": "IPY_MODEL_121f1fa4dea34f4c95098b9879b15206",
            "value": "Extracting: 100%"
          }
        },
        "bec5ac06dfdd4557a559fd32033567d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ec1d4ad7244441bcf3ac8e86a4e5f1",
            "max": 549,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b52de54d0a84821886cc3d64770c549",
            "value": 549
          }
        },
        "196af022b4e94ad894d194e33d83eb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42c0ada61d64805a4df6388ed8fb89e",
            "placeholder": "​",
            "style": "IPY_MODEL_6736c1be8bbf4fb6bafd1533651d89eb",
            "value": " 549/549 [00:01&lt;00:00, 332.27it/s]"
          }
        },
        "0b3b1230951f4160a01866047d0c13e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303e6f5cad92405090de073cd3291100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121f1fa4dea34f4c95098b9879b15206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83ec1d4ad7244441bcf3ac8e86a4e5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b52de54d0a84821886cc3d64770c549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "a42c0ada61d64805a4df6388ed8fb89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6736c1be8bbf4fb6bafd1533651d89eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Gathering and Installations"
      ],
      "metadata": {
        "id": "euootdVirZt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download the wiki103 dataset\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com\n",
        "# Unzip the file\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive':\n",
        "    # For kaggle notebook\n",
        "    !unzip /kaggle/working/wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com\n",
        "else:\n",
        "    !unzip wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com"
      ],
      "metadata": {
        "id": "QfKOSqomrWan",
        "outputId": "273b130f-aa6e-4ff6-cce0-eb72fd10e612",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:24:34.367407Z",
          "iopub.execute_input": "2023-12-10T13:24:34.367795Z",
          "iopub.status.idle": "2023-12-10T13:24:54.751802Z",
          "shell.execute_reply.started": "2023-12-10T13:24:34.367751Z",
          "shell.execute_reply": "2023-12-10T13:24:54.750782Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-11 15:56:31--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.88.254, 52.217.171.240, 54.231.204.168, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.88.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  47.9MB/s    in 4.3s    \n",
            "\n",
            "2023-12-11 15:56:36 (42.9 MB/s) - ‘wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchinfo"
      ],
      "metadata": {
        "id": "SulxhPxSsJMD",
        "outputId": "81382eef-1850-414f-c9ee-8079740d09ef",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:24:54.753640Z",
          "iopub.execute_input": "2023-12-10T13:24:54.753948Z",
          "iopub.status.idle": "2023-12-10T13:25:07.067079Z",
          "shell.execute_reply.started": "2023-12-10T13:24:54.753920Z",
          "shell.execute_reply": "2023-12-10T13:25:07.065925Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PEzkp1W6rhYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, math, random, transformers, matplotlib.pyplot as plt, torch.nn.functional as F\n",
        "from torch import optim, nn\n",
        "from transformers import BertTokenizerFast\n",
        "from torchinfo import summary\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset"
      ],
      "metadata": {
        "id": "IW6Rt6Zprjjn",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:07.068643Z",
          "iopub.execute_input": "2023-12-10T13:25:07.068968Z",
          "iopub.status.idle": "2023-12-10T13:25:11.470802Z",
          "shell.execute_reply.started": "2023-12-10T13:25:07.068938Z",
          "shell.execute_reply": "2023-12-10T13:25:11.469988Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Tokenizer"
      ],
      "metadata": {
        "id": "ib8lnTujsZ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "bert = False\n",
        "if bert:\n",
        "  tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# In fine tuning phase. [INS] [/INS] tokens are to be added.\n",
        "# This can be done by tokenizer.add_tokens([list of tokens])\n",
        "# Then the model can be updated as model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "VN9ohn5prsSe",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:11.472954Z",
          "iopub.execute_input": "2023-12-10T13:25:11.473336Z",
          "iopub.status.idle": "2023-12-10T13:25:15.104554Z",
          "shell.execute_reply.started": "2023-12-10T13:25:11.473311Z",
          "shell.execute_reply": "2023-12-10T13:25:15.103608Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self,vocab_size = 1000,special_tokens=[\"<PAD>\",\"<START>\",\"<OOV>\",\"<END>\"]):\n",
        "        self.vocab_size = vocab_size\n",
        "        # Special Tokens should be in same order\n",
        "        self.token_dict = dict(zip(special_tokens, range(len(special_tokens))))\n",
        "\n",
        "    def fits_on_texts(self,texts):\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            word_counts.update([word.lower() for word in text.split()])\n",
        "\n",
        "        for word,_ in word_counts.most_common(self.vocab_size):\n",
        "            if len(self.token_dict) < self.vocab_size:\n",
        "                self.token_dict[word] = len(self.token_dict)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def texts_to_sequences(self,texts):\n",
        "        seq_list = []\n",
        "        for text in texts:\n",
        "            seq = []\n",
        "            for word in text.split():\n",
        "                seq.append(self.token_dict.get(word.lower(),2))  # 2 is the <OOV> token\n",
        "            seq_list.append(seq.copy())\n",
        "        return seq_list\n",
        "\n",
        "    def sequences_to_texts(self,seq_list):\n",
        "        texts = []\n",
        "        temp_token_dict = {value: key for key,value in self.token_dict.items()}\n",
        "        for seq in seq_list:\n",
        "            text = \"\"\n",
        "            for token in seq:\n",
        "                # Assumed that we are getting corrected sequences.\n",
        "                if token not in (0,1,2,3):  text += temp_token_dict.get(token,\"<UNK>\") + \" \"\n",
        "            texts.append(text.strip())\n",
        "        return texts\n",
        "\n",
        "    def token_of(self,word):\n",
        "        return self.token_dict.get(word,2)  # 2 is the <OOV> token\n",
        "\n",
        "    def value_of(self,token):\n",
        "        for key,value in self.token_dict.items():\n",
        "            if (token == value):  return key\n",
        "        return \"<UNK>\""
      ],
      "metadata": {
        "id": "ROSFvWo2RPUf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper params"
      ],
      "metadata": {
        "id": "Y-w9yM03rnNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {\n",
        "    \"seq_len\": 5, \"vocab_size\" : tokenizer.vocab_size if bert else 10_000,  # Vocab_size here is a temporary\n",
        "    \"d_model\" : 60, \"d_ff\" : 720, \"num_heads\" : 6, \"dropout\" : 0.15,\n",
        "    \"num_layers\" : 1,\n",
        "    \"epochs\" : 10, \"batch_size\" : 5120,\n",
        "}\n",
        "\n",
        "print(f'Vocab Size is :{hparams[\"vocab_size\"]}')\n",
        "\n",
        "# Experiment with num_layers : 1, dropout : 0.1, d_model : 60, d_ff : 360 (hopping to have more memory)"
      ],
      "metadata": {
        "id": "oW_SBd4Nro7Q",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.105779Z",
          "iopub.execute_input": "2023-12-10T13:25:15.106079Z",
          "iopub.status.idle": "2023-12-10T13:25:15.111731Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.106054Z",
          "shell.execute_reply": "2023-12-10T13:25:15.110887Z"
        },
        "trusted": true,
        "outputId": "73821afe-003f-4494-bdda-2f5aa57bd9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size is :10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the paragraphs from the wiki103 dataset"
      ],
      "metadata": {
        "id": "8Q96KaMW0JRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The headings are like\n",
        "=<text>=\n",
        "Basically it means the if we use this as seperator.\n",
        "We can get the resultant string as meaningful context.\n",
        "\n",
        "We then use that context word by word to get the next word.\n",
        "\"\"\"\n",
        "from collections import Counter\n",
        "\n",
        "def wiki103_data_extraction(path=\"wikitext-103-raw/\", files=[\"train\", 'test', 'valid']):\n",
        "    return \"\".join([open(f\"{path}wiki.{filename}.raw\", 'r', encoding='utf-8').read() for filename in files])\n",
        "\n",
        "def wiki103_pre_processing(content, start_index = 0, length = None):\n",
        "    # Split the contexts\n",
        "    context,context_list = \"\",[]\n",
        "    for line in content.split(\"\\n\"):\n",
        "        if (len(line) > 3):  # Removing the empty lines\n",
        "            if line[1] == '=' and line[-2] == '=' and context != \"\":\n",
        "                # Heading found\n",
        "                if start_index != 0: start_index -= 1\n",
        "                else:\n",
        "                   context_list.append(context)\n",
        "                   if length is not None and len(context_list) >= length:\n",
        "                      return context_list\n",
        "                context = \"\"\n",
        "            else:   context += line\n",
        "    context_list.append(context)   # For last context\n",
        "    return context_list"
      ],
      "metadata": {
        "id": "96q4z-Zm0P6M",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.113331Z",
          "iopub.execute_input": "2023-12-10T13:25:15.113676Z",
          "iopub.status.idle": "2023-12-10T13:25:15.127393Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.113643Z",
          "shell.execute_reply": "2023-12-10T13:25:15.126473Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting paragraphs to inputs and targets"
      ],
      "metadata": {
        "id": "WeN9oUo14H-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def single_token_encode_bert(inp):\n",
        "#   text, seq_len, toTensor = inp\n",
        "#   token_seq =  tokenizer.encode(text, add_special_tokens = True)\n",
        "\n",
        "#   inp_list, target_list = [], []\n",
        "#   for i in range(2, len(token_seq)):\n",
        "#     # Ignore the [CLS] and do not consider the last token input.\n",
        "#     seq, target = token_seq[:i], token_seq[i]\n",
        "\n",
        "#     # Check for the input seq_len\n",
        "#     if len(seq) > seq_len:  seq = seq[-seq_len:]\n",
        "#     else: seq += [0]*max(0,seq_len - len(seq))\n",
        "\n",
        "#     inp_list.append(seq)\n",
        "#     target_list.append(target)\n",
        "#   return ((torch.tensor(inp_list) if toTensor else inp_list),\n",
        "#           (torch.tensor(target_list) if toTensor else target_list))\n",
        "\n",
        "# def get_tokens_slowly_bert(texts, seq_len, toTensor = True):\n",
        "#     inp_dataset, target_dataset = [], []\n",
        "#     for text in tqdm(texts,desc='Extracting',colour = 'green'):\n",
        "#         inp_data, target_data = single_token_encode_bert((text,seq_len, toTensor))\n",
        "#         inp_dataset.extend(inp_data)\n",
        "#         target_dataset.extend(target_data)\n",
        "\n",
        "#     print('Converting to tensors')\n",
        "#     return [(torch.stack(inp_dataset) if toTensor else inp_dataset),\n",
        "#             (torch.tensor(target_dataset) if toTensor else target_dataset)]\n",
        "\n",
        "# def get_slicing_tokens_slowly_bert(texts, dataset, seq_len, toTensor = True, batch_size = 5000):\n",
        "#   prev_index = 0\n",
        "\n",
        "#   while (prev_index < len(texts)):\n",
        "#     print(f'Iteration:{int(prev_index/batch_size)+1}')\n",
        "#     res = get_tokens_slowly_bert(texts[prev_index:prev_index + batch_size], hparams['seq_len'], toTensor)\n",
        "#     dataset[0].append(res[0])\n",
        "#     dataset[1].append(res[1])\n",
        "#     del res\n",
        "#     prev_index += batch_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.128489Z",
          "iopub.execute_input": "2023-12-10T13:25:15.128837Z",
          "iopub.status.idle": "2023-12-10T13:25:15.140867Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.128805Z",
          "shell.execute_reply": "2023-12-10T13:25:15.140005Z"
        },
        "trusted": true,
        "id": "IGD9HfgrgvVz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_token_encode(inp):\n",
        "  text, seq_len, toTensor, tokenizer = inp\n",
        "  token_seq =  tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "  inp_list, target_list = [], []\n",
        "  for i in range(2, len(token_seq)):\n",
        "    # Ignore the [CLS] and do not consider the last token input.\n",
        "    seq, target = token_seq[:i], token_seq[i]\n",
        "\n",
        "    # Check for the input seq_len\n",
        "    if len(seq) > seq_len:  seq = seq[-seq_len:]\n",
        "    else: seq += [0]*max(0,seq_len - len(seq))\n",
        "\n",
        "    inp_list.append(seq)\n",
        "    target_list.append(target)\n",
        "  return ((torch.tensor(inp_list) if toTensor else inp_list),\n",
        "          (torch.tensor(target_list) if toTensor else target_list))\n",
        "\n",
        "def get_tokens_slowly(tokenizer, texts, seq_len, toTensor = True):\n",
        "    inp_dataset, target_dataset = [], []\n",
        "    for text in tqdm(texts,desc='Extracting',colour = 'green'):\n",
        "        inp_data, target_data = single_token_encode((text,seq_len, toTensor, tokenizer))\n",
        "        inp_dataset.extend(inp_data)\n",
        "        target_dataset.extend(target_data)\n",
        "\n",
        "    print('Converting to tensors')\n",
        "    return [(torch.stack(inp_dataset) if toTensor else inp_dataset),\n",
        "            (torch.tensor(target_dataset) if toTensor else target_dataset)]\n",
        "\n",
        "def get_slicing_tokens_slowly(tokenizer, texts, dataset, seq_len, toTensor = True, batch_size = 5000):\n",
        "  prev_index = 0\n",
        "\n",
        "  while (prev_index < len(texts)):\n",
        "    print(f'Iteration:{int(prev_index/batch_size)+1}')\n",
        "    res = get_tokens_slowly(tokenizer, texts[prev_index:prev_index + batch_size], hparams['seq_len'], toTensor)\n",
        "    dataset[0].append(res[0])\n",
        "    dataset[1].append(res[1])\n",
        "    del res\n",
        "    prev_index += batch_size"
      ],
      "metadata": {
        "id": "NIwEZiDAR1dE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from multiprocessing import Pool\n",
        "\n",
        "# def get_tokens(texts, seq_len, toTensor = True, num_processes = 4):\n",
        "#   inp_dataset, target_dataset = [], []\n",
        "\n",
        "#   with Pool(num_processes) as pool:\n",
        "#     processed_data = list(tqdm(pool.imap(single_token_encode,\n",
        "#                                          [(text, seq_len, toTensor) for text in texts]),\n",
        "#                                 total=len(texts), desc='Processing', colour='green'))\n",
        "#   for data in processed_data:\n",
        "#     inp_dataset.extend(data[0])\n",
        "#     target_dataset.extend(data[1])\n",
        "\n",
        "#   return [(torch.stack(inp_dataset) if toTensor else inp_dataset),\n",
        "#           (torch.tensor(target_dataset) if toTensor else target_dataset)]"
      ],
      "metadata": {
        "id": "aAH86Zz34L3v",
        "execution": {
          "iopub.status.busy": "2023-12-06T18:09:24.928587Z",
          "iopub.execute_input": "2023-12-06T18:09:24.928863Z",
          "iopub.status.idle": "2023-12-06T18:09:24.941063Z",
          "shell.execute_reply.started": "2023-12-06T18:09:24.928819Z",
          "shell.execute_reply": "2023-12-06T18:09:24.940149Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset for effective RAM management in kaggle"
      ],
      "metadata": {
        "id": "s70cA4p4gvVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset to mimic the action of on fly data loading with RAM usage.\n",
        "    \"\"\"\n",
        "    total_examples = 0\n",
        "    dataset = [[],[]]\n",
        "    def __init__(self, texts, vocab_size, seq_len, batch_size):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        ----------\n",
        "        texts [List[str]] : The sequences of paragraphs\n",
        "        seq_len : The sequence length needed.\n",
        "        batch_size : batches to be processed\n",
        "        \"\"\"\n",
        "        self.tokenizer = Tokenizer(vocab_size = vocab_size)\n",
        "        self.tokenizer.fits_on_texts(texts)\n",
        "        get_slicing_tokens_slowly(self.tokenizer, texts, self.dataset, seq_len, batch_size = batch_size)\n",
        "        self.total_examples += sum([t.shape[0] for t in self.dataset[0]])\n",
        "    def __len__(self):\n",
        "        return self.total_examples\n",
        "\n",
        "    def update_dataset(self, texts, seq_len, batch_size):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        ----------\n",
        "        texts [List[str]] : The sequences of paragraphs\n",
        "        seq_len : The sequence length needed.\n",
        "        batch_size : batches to be processed\n",
        "        \"\"\"\n",
        "        get_slicing_tokens_slowly(texts, self.dataset, seq_len, batch_size = batch_size)\n",
        "        self.total_examples += sum([t.shape[0] for t in self.dataset[0]])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        ----------\n",
        "        idx : Actual index of the tensor\n",
        "        \"\"\"\n",
        "        stk_idx = 0\n",
        "        while (idx >= self.dataset[0][stk_idx].shape[0]):\n",
        "            idx -= self.dataset[0][stk_idx].shape[0]\n",
        "        return self.dataset[0][stk_idx][idx], self.dataset[1][stk_idx][idx]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.141876Z",
          "iopub.execute_input": "2023-12-10T13:25:15.142192Z",
          "iopub.status.idle": "2023-12-10T13:25:15.155284Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.142160Z",
          "shell.execute_reply": "2023-12-10T13:25:15.154544Z"
        },
        "trusted": true,
        "id": "PTXr5kHLgvVz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "XaFVD3JuND0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Layers"
      ],
      "metadata": {
        "id": "1bu1h0MWN8tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model, self.vocab_size = d_model, vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "    def forward(self,x):   # x: (batch_size, seq_len)\n",
        "        return self.embedding(x)*math.sqrt(self.d_model)  # (batch_size, seq_len, d_model)\n",
        "        # math.sqrt() is suggested from research paper.\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len=1000, dropout = 0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(0.1)  # A dropout layer to prevent overfitting\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)  # Empty Matrix with zeros\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (max_seq_len) -> (max_seq_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Filling even indices with sin function\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Filling odd indices with cos function\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        # Add positional encodings to the input\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self,eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps  # CPU and GPU cannot go to very very small precisions.\n",
        "        self.alpha = nn.Parameter(torch.ones(1))   # Weight (Multiplied)\n",
        "        self.bias = nn.Parameter(torch.zeros(1))   # Bias (Added)\n",
        "\n",
        "    def forward(self,x):  # x: (batch_size, seq_len, d_model)\n",
        "        mean = x.mean(dim = -1, keepdim = True)\n",
        "        std = x.std(dim = -1, keepdim = True)\n",
        "        return self.alpha*(x - mean)/(std + self.eps) + self.bias\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)  # W1 and B1\n",
        "        self.linear2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size,seq_len,d_model) -> (batch_size,seq_len,d_ff) -> (batch_size,)\n",
        "        return self.linear2(self.dropout(self.linear1(x)))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=True)  # Wo matrix\n",
        "        assert d_model % num_heads == 0  # Ensure d_model is divisible by num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "    def forward(self, q, k, v, causal_mask=False):\n",
        "        batch_size, seq_len_q, d_model = q.shape\n",
        "        _, seq_len_kv, _ = k.shape\n",
        "\n",
        "        interim_shape_q = (batch_size, seq_len_q, self.num_heads, self.head_dim)\n",
        "        interim_shape_kv = (batch_size, seq_len_kv, self.num_heads, self.head_dim)\n",
        "\n",
        "        q = q.view(interim_shape_q).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_len_q, head_dim)\n",
        "        k = k.view(interim_shape_kv).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_len_kv, head_dim)\n",
        "        v = v.view(interim_shape_kv).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_len_kv, head_dim)\n",
        "\n",
        "        weight = q @ k.transpose(-1, -2)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_kv)\n",
        "        if causal_mask:\n",
        "            weight.masked_fill_(torch.ones_like(weight, dtype=torch.bool).triu(1), -torch.inf)\n",
        "        weight = F.softmax(weight / math.sqrt(self.head_dim), dim=-1)\n",
        "        self.attention_scores = weight\n",
        "\n",
        "        output = weight @ v  # Shape: (batch_size, num_heads, seq_len_q, head_dim)\n",
        "        # Reshape back to (batch_size, seq_len_q, d_model)\n",
        "        output = output.transpose(1, 2).reshape(batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return self.out_proj(output)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.mha = MultiHeadAttention(d_model=d_model,num_heads=num_heads,dropout=dropout)\n",
        "  def forward(self,x,causal_mask=False):      # x: (Batch_size, Seq_Len, Dim)\n",
        "      return self.mha(x,x,x,causal_mask)    # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    # The last linear and softmax layer of Transformer.\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x): # x: (Batch_size,seq_len,d_model)\n",
        "        # The output should be (Batch_size,seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.proj(x), dim = -1)"
      ],
      "metadata": {
        "id": "e7ESy6zONG7N",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.156564Z",
          "iopub.execute_input": "2023-12-10T13:25:15.156897Z",
          "iopub.status.idle": "2023-12-10T13:25:15.182416Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.156866Z",
          "shell.execute_reply": "2023-12-10T13:25:15.181519Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Blocks"
      ],
      "metadata": {
        "id": "TAz-VfBKODX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attn = SelfAttention(d_model, num_heads, dropout)\n",
        "        self.norm1, self.norm2, self.norm3 = [LayerNormalization() for _ in range(3)]\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.norm1(x + self.self_attn(x, causal_mask = True))   # Self Attention and Layer Normalization (Add and Norm)\n",
        "        return self.norm3(out1 + self.feed_forward(out1))              # Feed Forward Layer Normalization (Add and Norm)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.input_embedding = InputEmbeddings(d_model, vocab_size)\n",
        "        self.pos_encodings = PositionalEncoding(d_model, seq_len, dropout)\n",
        "        self.decoder = nn.ModuleList([DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.out_proj = ProjectionLayer(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        # Embedding and positional encoding\n",
        "        input_enc = self.pos_encodings(self.input_embedding(input_seq))\n",
        "\n",
        "        # Decoder blocks\n",
        "        for decoder_block in self.decoder:\n",
        "            input_enc = decoder_block(input_enc)\n",
        "\n",
        "        input_enc = self.out_proj(input_enc)\n",
        "        return input_enc"
      ],
      "metadata": {
        "id": "mqty5HT2OFH7",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.185900Z",
          "iopub.execute_input": "2023-12-10T13:25:15.186262Z",
          "iopub.status.idle": "2023-12-10T13:25:15.197509Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.186238Z",
          "shell.execute_reply": "2023-12-10T13:25:15.196655Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verifying the model architecture"
      ],
      "metadata": {
        "id": "dUoRhzmLQlfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(hparams['vocab_size'], hparams['seq_len'], hparams['d_model'],\n",
        "                     hparams['num_heads'], hparams['d_ff'], hparams['num_layers'],\n",
        "                     hparams['dropout'])\n",
        "summary(model,input_sizes=(hparams['batch_size'],hparams['seq_len']))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.198702Z",
          "iopub.execute_input": "2023-12-10T13:25:15.199071Z",
          "iopub.status.idle": "2023-12-10T13:25:15.991152Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.199015Z",
          "shell.execute_reply": "2023-12-10T13:25:15.990263Z"
        },
        "trusted": true,
        "id": "cfwsF706gvV1",
        "outputId": "7ec0065f-742b-4147-8f83-4b3df0147e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "Transformer                                   --\n",
              "├─InputEmbeddings: 1-1                        --\n",
              "│    └─Embedding: 2-1                         600,000\n",
              "├─PositionalEncoding: 1-2                     --\n",
              "│    └─Dropout: 2-2                           --\n",
              "├─ModuleList: 1-3                             --\n",
              "│    └─DecoderBlock: 2-3                      --\n",
              "│    │    └─SelfAttention: 3-1                3,660\n",
              "│    │    └─LayerNormalization: 3-2           2\n",
              "│    │    └─LayerNormalization: 3-3           2\n",
              "│    │    └─LayerNormalization: 3-4           2\n",
              "│    │    └─FeedForward: 3-5                  87,180\n",
              "├─ProjectionLayer: 1-4                        --\n",
              "│    └─Linear: 2-4                            610,000\n",
              "======================================================================\n",
              "Total params: 1,300,846\n",
              "Trainable params: 1,300,846\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5hVTSs1WR6Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, optimizer, hparams, trainLoader, validLoader, evalPerEpoch = False, Device = 'cpu'):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "    model.cuda() if Device == 'cuda' else model.cpu()\n",
        "\n",
        "    for epoch in range(hparams[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss, acc,total_count = 0.0, 0.0, 0.0\n",
        "        pbar = tqdm(trainLoader,desc='Training', colour = 'green', leave = False)\n",
        "        for input_batch, targets_batch in pbar:\n",
        "            # Get the data to the Device\n",
        "            input_batch, targets_batch = input_batch.to(Device), targets_batch.to(Device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_batch)\n",
        "            # Calculate the loss\n",
        "            # The last token of output should be the next word of the sequence.\n",
        "            # For getting an Auto Completion model\n",
        "            loss = criterion(output[:,-1], targets_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Update the loss\n",
        "            total_loss += loss.item()\n",
        "            acc += (output[:,-1].argmax(-1) == targets_batch).sum().item()\n",
        "            total_count += input_batch.shape[0]\n",
        "            pbar.set_postfix({'loss':total_loss/len(trainLoader),'acc':acc*100/total_count})\n",
        "            del input_batch, targets_batch, output, loss\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"Epoch: {epoch+1}, Loss: {(total_loss/len(trainLoader)):.4f}, acc: {(acc*100/(len(trainLoader)*hparams['batch_size'])):.4f}, \",end=\"\")\n",
        "        model.eval()\n",
        "        total_loss, acc = 0.0, 0.0\n",
        "        pbar = tqdm(validLoader, desc = 'Validating', leave = False)\n",
        "        for input_batch, targets_batch in pbar:\n",
        "            # Get the data to the Device\n",
        "            input_batch, targets_batch = input_batch.to(Device), targets_batch.to(Device)\n",
        "\n",
        "            # Clearing the gradiants\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Find the outputs\n",
        "            output = model(input_batch)\n",
        "\n",
        "            # Calculate the loss\n",
        "            # The last token of output should be the next word of the sequence.\n",
        "            # For getting an Auto Generation model\n",
        "            loss = criterion(output[:,-1], targets_batch)\n",
        "\n",
        "            # Update the loss\n",
        "            total_loss += loss.item()\n",
        "            acc += (output[:,-1].argmax(-1) == targets_batch).sum().item()\n",
        "            pbar.set_postfix({'loss':total_loss/len(validLoader),'acc':acc*100/(len(validLoader)*hparams['batch_size'])})\n",
        "            del input_batch, targets_batch, output, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"\\tVal_Loss: {(total_loss/len(validLoader)):.4f}, Val_Acc: {(acc*100/(len(validLoader)*hparams['batch_size'])):.4f}\")\n",
        "    # Set the model to normal mode.\n",
        "    model.cpu()"
      ],
      "metadata": {
        "id": "cwE_qJDtR7eb",
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:15.992577Z",
          "iopub.execute_input": "2023-12-10T13:25:15.992884Z",
          "iopub.status.idle": "2023-12-10T13:25:16.006352Z",
          "shell.execute_reply.started": "2023-12-10T13:25:15.992858Z",
          "shell.execute_reply": "2023-12-10T13:25:16.005395Z"
        },
        "trusted": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "R5sPVocYThdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hparams['batch_size'] = 128"
      ],
      "metadata": {
        "trusted": true,
        "id": "5fBYarAegvV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "prlWcd4CgvV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "# Get train data as batches so as to reduce the momentary overload on RAM\n",
        "train = wiki103_pre_processing(wiki103_data_extraction(files=['train']), length = 5_000)\n",
        "print(f'Extracted {len(train)} paras')\n",
        "train_dataset = customDataset(train, hparams['vocab_size'], hparams['seq_len'], batch_size = 5_000)\n",
        "print(f'Train Dataset after 30K: {train_dataset.total_examples}')\n",
        "\n",
        "# train = wiki103_pre_processing(wiki103_data_extraction(files=['train']), start_index = 75000, length = 75000)\n",
        "# train_dataset.update_dataset(train, hparams['seq_len'], batch_size = 10000)\n",
        "# print(f'Train Dataset after 200K: {train_dataset.total_examples}')\n",
        "print(f'Time taken:{time() - start_time}')\n",
        "del train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:25:16.007495Z",
          "iopub.execute_input": "2023-12-10T13:25:16.007819Z",
          "iopub.status.idle": "2023-12-10T13:28:40.960273Z",
          "shell.execute_reply.started": "2023-12-10T13:25:16.007796Z",
          "shell.execute_reply": "2023-12-10T13:28:40.959314Z"
        },
        "trusted": true,
        "id": "USQTKkQsgvV1",
        "outputId": "720a891c-8077-4c94-d4e4-c0418b221c03",
        "colab": {
          "referenced_widgets": [
            "76dfcc2386ea4e2a9622cc9e77b6b94a",
            "e1235bc806d14a9d994fdb94dd497f42",
            "9896b5c1aa48402ca3e5b9caeabe0960",
            "e44c8565cea044e99dd005bc8a7666ca",
            "182ac8acc1c84781b7d191d0c6004655",
            "099ca037e1c7460ab39f40c56ea7ee31",
            "1b271ac3514a4a5fb3afebbbeabb8a82",
            "b8c92d92c2424551bffa3b51796ecde4",
            "ba2eb5e1942e4ca7983a7754bc62e5b4",
            "2d56c0e6f23d48ec868595e7cccb57e9",
            "c7c2f65df7b94a578e077501f532efc3"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5000 paras\n",
            "Iteration:1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76dfcc2386ea4e2a9622cc9e77b6b94a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to tensors\n",
            "Train Dataset after 30K: 1784533\n",
            "Time taken:37.875691175460815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset.dataset[0]))\n",
        "train_dataset.tokenizer.vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-07T09:45:42.278813Z",
          "iopub.execute_input": "2023-12-07T09:45:42.279080Z",
          "iopub.status.idle": "2023-12-07T09:45:42.284863Z",
          "shell.execute_reply.started": "2023-12-07T09:45:42.279057Z",
          "shell.execute_reply": "2023-12-07T09:45:42.283864Z"
        },
        "trusted": true,
        "id": "OUC50h-ggvV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67b4c54-22ec-4df3-ba9e-48ea0c6dbaf8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data.\n",
        "# train = wiki103_pre_processing(wiki103_data_extraction(files=['train']), length = 140000)\n",
        "val = wiki103_pre_processing(wiki103_data_extraction(files=['valid']))\n",
        "print(f\"Data Extracted\\ntVal paras:{len(val)}\")\n",
        "\n",
        "# Convert it to datasets\n",
        "# train = get_slicing_tokens_slowly(train, hparams['seq_len'], toTensor = True)\n",
        "val = get_tokens_slowly(train_dataset.tokenizer, val, hparams['seq_len'], toTensor = True)\n",
        "print(f\"Converted into datasets\\n\\tVal examples:{val[0].shape}\")\n",
        "\n",
        "# Prepare dataloaders\n",
        "train = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
        "val = DataLoader(TensorDataset(val[0],val[1]), batch_size=hparams['batch_size'], shuffle=True)\n",
        "print(\"Dataloaders are prepared\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:28:40.962908Z",
          "iopub.execute_input": "2023-12-10T13:28:40.963271Z",
          "iopub.status.idle": "2023-12-10T13:28:44.243565Z",
          "shell.execute_reply.started": "2023-12-10T13:28:40.963236Z",
          "shell.execute_reply": "2023-12-10T13:28:44.242659Z"
        },
        "trusted": true,
        "id": "QPqVZufCgvV2",
        "outputId": "436818d6-46d6-4434-aa36-53df200893fd",
        "colab": {
          "referenced_widgets": [
            "7063e0500efa45d38f981762aa385cf2",
            "cd9384c4299c40c8920b31de9f942c18",
            "fd85e92c3ee842f18e8f94752d6f7df7",
            "ea462aaac31841a6b0e13677f8fa0edc",
            "3a35e4be59804598ab67ab10e5b6c2b7",
            "8eb3999439bc40f482601e923cea9f6c",
            "493e7e24c86c47e881fde5917ba4a5e7",
            "881ff3efd165440baeb1a45ebb25e7a7",
            "ca130dd156e34173a05ce06b434a21e7",
            "3a834551562c41938e0e9f8a6a569383",
            "0b14ebd25ce7469aac24db8314283aa3"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Extracted\n",
            "tVal paras:549\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting:   0%|          | 0/549 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7063e0500efa45d38f981762aa385cf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to tensors\n",
            "Converted into datasets\n",
            "\tVal examples:torch.Size([208842, 5])\n",
            "Dataloaders are prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model and Optimizer instance\n",
        "model = Transformer(hparams['vocab_size'], hparams['seq_len'], hparams['d_model'],\n",
        "                    hparams['num_heads'], hparams['d_ff'], hparams['num_layers'],\n",
        "                    hparams['dropout'])\n",
        "# adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer = AdamWarmup(model_size = hparams['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:28:44.245528Z",
          "iopub.execute_input": "2023-12-10T13:28:44.245905Z",
          "iopub.status.idle": "2023-12-10T13:28:44.891839Z",
          "shell.execute_reply.started": "2023-12-10T13:28:44.245870Z",
          "shell.execute_reply": "2023-12-10T13:28:44.890940Z"
        },
        "trusted": true,
        "id": "Mhj4zCQigvV2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(),lr=0.02,betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# lr = 0.03 (3x more in hope of gain max accuracy in epoch 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T13:28:44.893218Z",
          "iopub.execute_input": "2023-12-10T13:28:44.893913Z",
          "iopub.status.idle": "2023-12-10T13:28:44.900149Z",
          "shell.execute_reply.started": "2023-12-10T13:28:44.893877Z",
          "shell.execute_reply": "2023-12-10T13:28:44.899188Z"
        },
        "trusted": true,
        "id": "oTns1dXngvV2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the appropriate device\n",
        "Device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Selecting device as {Device}')\n",
        "\n",
        "# Train the model.\n",
        "print(\"Training the model\")\n",
        "train_step(model, optimizer, hparams, train, val, Device = Device)"
      ],
      "metadata": {
        "id": "z_-YIy_6TjDW",
        "outputId": "bc19c78a-21a2-4513-f052-e6333823e6db",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T13:28:44.901423Z",
          "iopub.execute_input": "2023-12-10T13:28:44.901710Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "77948ed1014f4b35ab5eae831d6a76c7",
            "521676a1c4964d9886bb0ed1ab1c2d5c",
            "bb4e2750b1cb49c5b38d42fb2a8bada4",
            "ced8c6c2e12b4dc0ac65d56d3ac970f8",
            "fba601e774054044b7ebeecb91def517",
            "f99ccc3f273646aa851a6cbc13a28aeb",
            "e89068fd94274e2fa031cd31399d7d98",
            "26a1015182624b9ca0c62f60dab08b04",
            "cba3ce5ee9e6456d8616b44278adb564",
            "d478d09018be4d77a084eb510651f9a8",
            "fd481410ff24402bba261ef0e7cc16a3"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting device as cpu\n",
            "Training the model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/349 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77948ed1014f4b35ab5eae831d6a76c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "WSaVZA9WgvV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = wiki103_pre_processing(wiki103_data_extraction(files=['valid']))\n",
        "val = get_tokens_slowly(train_dataset.tokenizer,val, hparams['seq_len'], toTensor = True)"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T08:18:54.176421Z",
          "iopub.execute_input": "2023-12-10T08:18:54.177270Z",
          "iopub.status.idle": "2023-12-10T08:18:58.287422Z",
          "shell.execute_reply.started": "2023-12-10T08:18:54.177234Z",
          "shell.execute_reply": "2023-12-10T08:18:58.286574Z"
        },
        "trusted": true,
        "id": "bKzLuDTVgvV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "793b7aeec0ae48519831d9d17e66ba7e",
            "f456707bfad44722becd160370ae6139",
            "bec5ac06dfdd4557a559fd32033567d6",
            "196af022b4e94ad894d194e33d83eb37",
            "0b3b1230951f4160a01866047d0c13e9",
            "303e6f5cad92405090de073cd3291100",
            "121f1fa4dea34f4c95098b9879b15206",
            "83ec1d4ad7244441bcf3ac8e86a4e5f1",
            "1b52de54d0a84821886cc3d64770c549",
            "a42c0ada61d64805a4df6388ed8fb89e",
            "6736c1be8bbf4fb6bafd1533651d89eb"
          ]
        },
        "outputId": "25bec2cd-081a-4f0b-993b-de6f0cd702ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting:   0%|          | 0/549 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "793b7aeec0ae48519831d9d17e66ba7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to tensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.cpu()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T08:18:58.304098Z",
          "iopub.execute_input": "2023-12-10T08:18:58.304433Z",
          "iopub.status.idle": "2023-12-10T08:18:58.316706Z",
          "shell.execute_reply.started": "2023-12-10T08:18:58.304405Z",
          "shell.execute_reply": "2023-12-10T08:18:58.315657Z"
        },
        "trusted": true,
        "id": "7Ro8wWtKgvV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da6c788-58ca-4a5b-86fa-abeab21a4646"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (input_embedding): InputEmbeddings(\n",
              "    (embedding): Embedding(10000, 30)\n",
              "  )\n",
              "  (pos_encodings): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): ModuleList(\n",
              "    (0-1): 2 x DecoderBlock(\n",
              "      (self_attn): SelfAttention(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (norm2): LayerNormalization()\n",
              "      (norm3): LayerNormalization()\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear1): Linear(in_features=30, out_features=180, bias=True)\n",
              "        (linear2): Linear(in_features=180, out_features=30, bias=True)\n",
              "        (dropout): Dropout(p=0.15, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out_proj): ProjectionLayer(\n",
              "    (proj): Linear(in_features=30, out_features=10000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline cell before running this.\n",
        "import random\n",
        "\n",
        "match_count = 0\n",
        "sample_size = 20\n",
        "for iter in range(sample_size):\n",
        "  i = random.randint(0,(val[1].shape)[0])\n",
        "  test_input, test_target = val[0][i], val[1][i]\n",
        "\n",
        "  pred = model(test_input.unsqueeze(0)).argmax(-1)\n",
        "  print(pred.shape)\n",
        "  print(f\"\\tMatch:{test_target == pred[0][-1]}\")\n",
        "  match_count += (test_target == pred[0][-1]).item()\n",
        "#   print(f\"Acutal Output:{tokenizer.sequences_to_texts(pred.tolist())[0]}\\n\")\n",
        "\n",
        "print(f\"Accuracy : {match_count*100/sample_size}\\n\\nNow Context is empty:\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T08:19:08.046404Z",
          "iopub.execute_input": "2023-12-10T08:19:08.047233Z",
          "iopub.status.idle": "2023-12-10T08:19:08.116897Z",
          "shell.execute_reply.started": "2023-12-10T08:19:08.047191Z",
          "shell.execute_reply": "2023-12-10T08:19:08.115924Z"
        },
        "trusted": true,
        "id": "qYWPjZKhgvV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d1b6d8-64b4-44b9-b6e1-0b5c94694551"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:True\n",
            "torch.Size([1, 5])\n",
            "\tMatch:True\n",
            "torch.Size([1, 5])\n",
            "\tMatch:True\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:True\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:True\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "torch.Size([1, 5])\n",
            "\tMatch:False\n",
            "Accuracy : 25.0\n",
            "\n",
            "Now Context is empty:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with user input"
      ],
      "metadata": {
        "id": "Yy03Y0_ygvV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_context = \"\"\n",
        "# Device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "while True:\n",
        "    ask = input('User:').lower()\n",
        "    if (ask == 'exit'):\n",
        "        break\n",
        "    answer = \"\"\n",
        "    for i in range(5):\n",
        "        # (batch_size = 1, seq_len = hparams['inp_seq_len'],)\n",
        "        model_inp = torch.tensor(tokenizer.encode(ask, add_special_tokens = True)[-hparams['seq_len']:]).unsqueeze(0)\n",
        "        # This model variable holds the trained model. (defined in the training block)\n",
        "        model_out = model(model_inp).argmax(-1)\n",
        "        out_txt = tokenizer.decode(model_out[:,-1].item())\n",
        "        # Adding the data to context\n",
        "        ask += \" \" + out_txt\n",
        "        answer += \" \" + out_txt\n",
        "    print(f\"Model:{answer}\\n\")\n",
        "    ask = \"\""
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T08:19:18.292977Z",
          "iopub.execute_input": "2023-12-10T08:19:18.293661Z",
          "iopub.status.idle": "2023-12-10T08:20:06.481455Z",
          "shell.execute_reply.started": "2023-12-10T08:19:18.293625Z",
          "shell.execute_reply": "2023-12-10T08:20:06.480419Z"
        },
        "trusted": true,
        "id": "OFS3niGfgvV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "43bb3f94-11ce-4aa3-8cc7-0fb04de4bbf2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User:this is a\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-21f3d1ebdd29>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# This model variable holds the trained model. (defined in the training block)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Adding the data to context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-294629ca74d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Embedding and positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0minput_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Decoder blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-21f7d5755302>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# x: (batch_size, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# math.sqrt() is suggested from research paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "ECQYY8sQgvV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cpu()\n",
        "optimizer.cpu()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hVmGt68BgvWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save the model, optimizer, and hyperparameters\n",
        "model_path = '/kaggle/working/model.pt'\n",
        "optimizer_path = '/kaggle/working/optimizer.pth'\n",
        "hparams_path = '/kaggle/working/hyperparameters.json'\n",
        "\n",
        "# Save model state\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Save optimizer parameters\n",
        "torch.save(optimizer.state_dict(), optimizer_path)\n",
        "\n",
        "# Save hyperparameters\n",
        "with open(hparams_path, 'w') as f:\n",
        "    json.dump(hparams, f)"
      ],
      "metadata": {
        "trusted": true,
        "id": "P2E1IaMFgvWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_path = '/kaggle/working/optimizer_state_dict.pth'\n",
        "torch.save(optimizer.optimizer.state_dict(),optimizer_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4l_Nk5qGgvWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model"
      ],
      "metadata": {
        "id": "UNcHQg06gvWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "\n",
        "# Load hyperparameters\n",
        "# hparams_path = '/kaggle/input/base-model-false/hyperparameters.json'\n",
        "model_path = '/kaggle/input/model-training-3/model.pt'\n",
        "hparams_path = '/kaggle/input/model-training-3/hyperparameters.json'\n",
        "optimizer_path = '/kaggle/input/model-training-3/optimizer.pth'\n",
        "\n",
        "print('Getting the hyperparams')\n",
        "with open(hparams_path, 'r') as f:\n",
        "    hparams = json.load(f)\n",
        "\n",
        "# Load the model state\n",
        "# model_path = '/kaggle/input/base-model-false/model.pth'\n",
        "print('Loading the model')\n",
        "model = Transformer(hparams['vocab_size'], hparams['seq_len'], hparams['d_model'],\n",
        "                    hparams['num_heads'], hparams['d_ff'], hparams['num_layers'],\n",
        "                    hparams['dropout'])\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "print(summary(model,input_sizes=(hparams['batch_size'],hparams['seq_len'])))\n",
        "\n",
        "print('Getting the optimizer')\n",
        "optimizer = optim.AdamW(model.parameters(),lr=0.0001,betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer_path = '/kaggle/working/optimizer_state_dict.pth'\n",
        "optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "\n",
        "# Load optimizer parameters\n",
        "# optimizer_path = '/kaggle/input/base-model-false/optimizer_state_dict.pth'\n",
        "# adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer = AdamWarmup(model_size = hparams['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "# optimizer.optimizer.load_state_dict(torch.load(optimizer_path))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:35:16.659081Z",
          "iopub.execute_input": "2023-12-01T12:35:16.660207Z",
          "iopub.status.idle": "2023-12-01T12:35:20.167944Z",
          "shell.execute_reply.started": "2023-12-01T12:35:16.660164Z",
          "shell.execute_reply": "2023-12-01T12:35:20.167129Z"
        },
        "trusted": true,
        "id": "546A5Y85gvWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Preparing the validation set')\n",
        "val = wiki103_pre_processing(wiki103_data_extraction(files=['valid']))\n",
        "val = get_tokens_slowly(val, hparams['seq_len'], toTensor = True)\n",
        "val = DataLoader(TensorDataset(val[0],val[1]), batch_size=hparams['batch_size'], shuffle=True)\n",
        "\n",
        "Device = 'cuda' if torch.cuda else 'cpu'\n",
        "print(f'Device Selected as:{Device}')\n",
        "\n",
        "model.to(Device)\n",
        "model.eval()\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "total_loss, acc = 0.0, 0.0\n",
        "pbar = tqdm(val, desc = 'Validating', leave = False)\n",
        "for input_batch, targets_batch in pbar:\n",
        "    input_batch, targets_batch = input_batch.to(Device), targets_batch.to(Device)\n",
        "    # Clearing the gradiants\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Find the outputs\n",
        "    output = model(input_batch)\n",
        "\n",
        "    # Calculate the loss\n",
        "    # The last token of output should be the next word of the sequence.\n",
        "    # For getting an Auto Generation model\n",
        "    loss = criterion(output[:,-1], targets_batch)\n",
        "\n",
        "    # Update the loss\n",
        "    total_loss += loss.item()\n",
        "    acc += (output[:,-1].argmax(-1) == targets_batch).sum().item()\n",
        "    pbar.set_postfix({'loss':total_loss/len(val),'acc':acc/(len(val)*hparams['batch_size'])})\n",
        "    del input_batch, targets_batch, output, loss\n",
        "\n",
        "torch.cuda.empty_cache() if Device == 'cuda' else None\n",
        "print(f\"\\tVal_Loss: {(total_loss/len(val)):.4f}, Val_Acc: {(acc/(len(val)*hparams['batch_size'])):.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:35:27.063813Z",
          "iopub.execute_input": "2023-12-01T12:35:27.064173Z",
          "iopub.status.idle": "2023-12-01T12:35:50.333374Z",
          "shell.execute_reply.started": "2023-12-01T12:35:27.064142Z",
          "shell.execute_reply": "2023-12-01T12:35:50.332466Z"
        },
        "trusted": true,
        "id": "hY4RMjqZgvWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del val"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:35:50.335300Z",
          "iopub.execute_input": "2023-12-01T12:35:50.335934Z",
          "iopub.status.idle": "2023-12-01T12:35:50.340200Z",
          "shell.execute_reply.started": "2023-12-01T12:35:50.335897Z",
          "shell.execute_reply": "2023-12-01T12:35:50.339243Z"
        },
        "trusted": true,
        "id": "cHA8oTDygvWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del optimizer, model, hparams"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-07T09:38:49.386813Z",
          "iopub.execute_input": "2023-12-07T09:38:49.387185Z",
          "iopub.status.idle": "2023-12-07T09:38:49.391671Z",
          "shell.execute_reply.started": "2023-12-07T09:38:49.387154Z",
          "shell.execute_reply": "2023-12-07T09:38:49.390712Z"
        },
        "trusted": true,
        "id": "k3MEQaNygvWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-07T09:38:51.328888Z",
          "iopub.execute_input": "2023-12-07T09:38:51.329244Z",
          "iopub.status.idle": "2023-12-07T09:38:51.795946Z",
          "shell.execute_reply.started": "2023-12-07T09:38:51.329214Z",
          "shell.execute_reply": "2023-12-07T09:38:51.795173Z"
        },
        "trusted": true,
        "id": "5saS6bJPgvWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer_path = '/kaggle/working/optimizer_state_dict.pth'\n",
        "optimizer.load_state_dict(torch.load(optimizer_path))"
      ],
      "metadata": {
        "trusted": true,
        "id": "HQUbEWNwgvWB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}